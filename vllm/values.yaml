fullnameOverride: "vllm"

inferenceService:
  storage:
    storageUri: 'oci://quay.io/redhat-ai-services/modelcar-catalog:granite-3.1-8b-instruct'
  args:
    - "--max-model-len=23000"
    - "--gpu-memory-utilization=0.99"
  resources:
    limits:
      cpu: '4'
      memory: 20Gi
      nvidia.com/gpu: '1'
    requests:
      cpu: '2'
      memory: 16Gi
      nvidia.com/gpu: '1'
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Equal
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Equal
    value: "True"
